{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir \"logs/fit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Logociels\\Anaconda\\envs\\A5Deep\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Vincent\n",
    "import numpy as np\n",
    "import datetime\n",
    "#import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num‚ro de s‚rie du volume est B032-A377\n",
      "\n",
      " R‚pertoire de C:\\Users\\emile\\OneDrive - Association Cesi Viacesi mail\\A5\\IA\\Projet\n",
      "\n",
      "19/12/2023  10:42    <DIR>          .\n",
      "19/12/2023  10:35    <DIR>          ..\n",
      "19/12/2023  10:42    <DIR>          .ipynb_checkpoints\n",
      "19/12/2023  10:26             9ÿ686 liv1.ipynb\n",
      "               1 fichier(s)            9ÿ686 octets\n",
      "               3 R‚p(s)  11ÿ225ÿ591ÿ808 octets libres\n"
     ]
    }
   ],
   "source": [
    "#%cd C:\\Users\\emile\\OneDrive - Association Cesi Viacesi mail\\A5\\IA\\Projet\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33136 files belonging to 5 classes.\n",
      "Using 26509 files for training.\n",
      "Using 6627 files for validation.\n",
      "Found 8271 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set=tf.keras.preprocessing.image_dataset_from_directory(directory=\"Train\", color_mode='rgb', batch_size=32, image_size=(224, 224),\n",
    "    shuffle=True, subset=\"both\", seed=10, validation_split=0.2) #subset obligatoire si c'est aleatoire, on n'a pas le dossier val séparé\n",
    "test_set=tf.keras.preprocessing.image_dataset_from_directory(directory=\"Test\", color_mode='rgb', batch_size=32, image_size=(224, 224), \n",
    "    shuffle=True, seed=10)            #(224, 224) verifier apres l'input de l'archi utilisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_set.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(arch):\n",
    "  model= getattr(tf.keras.applications, arch) (include_top=False ,weights='imagenet', input_shape=(224, 224, 3))\n",
    "  model.trainable=False\n",
    "  inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "  x = model(inputs, training=True)\n",
    "\n",
    "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "  x = tf.keras.layers.Dense(512)(x) # use_bias=False  :dans certains cas, l'utilisation d'un biais peut entraîner des problèmes tels que le surapprentissage, une complexité accrue du\n",
    "  # modèle et des temps de formation plus longs. Par conséquent, désactiver l'utilisation d'un biais peut aider à régulariser le modèle et améliorer sa généralisation.\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.Activation('relu')(x)\n",
    "  x = tf.keras.layers.Dense(256)(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.Activation('relu')(x)\n",
    "  x = tf.keras.layers.Dense(128)(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "  new_model = tf.keras.Model(inputs, outputs)\n",
    "  new_model.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "  # Using tensorboard callback\n",
    "  log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_model_no_dropout\"\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "  log_dir=log_dir, \n",
    "  histogram_freq=1, \n",
    "  write_graph=True,\n",
    "  write_images=True, \n",
    "  #write_steps_per_second=True, \n",
    "  update_freq='epoch',\n",
    "  profile_batch=0, \n",
    "  embeddings_freq=0)\n",
    "\n",
    "  checkpoint= tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath=f\"C:/Users/bloum/Desktop/liv1/checkpoint{arch}.h5\",save_weights_only=True, monitor='val_accuracy',mode='max', save_best_only=True)\n",
    "  history=new_model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=[checkpoint])\n",
    "  ######################################################\n",
    "  CNN_Score = new_model.evaluate(train_ds)\n",
    "\n",
    "  print(\"    Train Loss: {:.5f}\".format(CNN_Score[0]))\n",
    "  print(\"Train Accuracy: {:.2f}%\".format(CNN_Score[1] * 100))\n",
    "  ########################################################\n",
    "  CNN_Score = new_model.evaluate(val_ds)\n",
    "\n",
    "  print(\"    Test Loss: {:.5f}\".format(CNN_Score[0]))\n",
    "  print(\"Test Accuracy: {:.2f}%\".format(CNN_Score[1] * 100))\n",
    "\n",
    "  loss = history.history[\"loss\"]\n",
    "  val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "  accuracy = history.history[\"accuracy\"]\n",
    "  val_accuracy = history.history[\"val_accuracy\"]\n",
    "\n",
    "  epochs = range(len(history.history[\"loss\"]))\n",
    "\n",
    "  plt.figure(figsize=(15,5))\n",
    "\n",
    "  #plot loss\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(epochs, loss, label = \"training_loss\")\n",
    "  plt.plot(epochs, val_loss, label = \"val_loss\")\n",
    "  plt.title(\"Loss\")\n",
    "  plt.xlabel(\"epochs\")\n",
    "  plt.legend()\n",
    "  #plot accuracy\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(epochs, accuracy, label = \"training_accuracy\")\n",
    "  plt.plot(epochs, val_accuracy, label = \"val_accuracy\")\n",
    "  plt.title(\"Accuracy\")\n",
    "  plt.xlabel(\"epochs\")\n",
    "  plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "829/829 [==============================] - ETA: 0s - loss: 0.1566 - accuracy: 0.9470"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "learning('ResNet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning('DenseNet201')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning('MobileNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning('VGG16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning('EfficientNetB7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
